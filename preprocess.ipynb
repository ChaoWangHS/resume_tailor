{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# default plot stying changes\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "colors = sns.color_palette('Set2',12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pw_file = 'credentials/pw.txt'\n",
    "if os.path.exists(pw_file): \n",
    "    with open(pw_file, 'r') as f:\n",
    "        email, indeed_pw = f.readline().strip().split(', ')\n",
    "        username, pia_pw = f.readline().strip().split(', ')\n",
    "        pub_ip, mongo_usr, mongo_usr_pw = f.readline().strip().split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to ec2 mongo client\n",
    "client = MongoClient('{0}:27017'.format(pub_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get reference to  resume_db\n",
    "db = client.resume_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# authenticate user for database\n",
    "db.authenticate(mongo_usr, mongo_usr_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull MongoDB into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, no_id=True):\n",
    "    '''\n",
    "    db: mongodb already connected and authenticated\n",
    "    collection: desired collection in db\n",
    "    query: query filter\n",
    "    no_id: include mongos _id (False) or not (True)\n",
    "    return => pandas dataframe\n",
    "    '''\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load data: 7.36710000038147s\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "\n",
    "# load database data into dataframe\n",
    "df = read_mongo(db, 'originals')\n",
    "\n",
    "print('Time to load data: {0}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City, State Abbreviation List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10066\n"
     ]
    }
   ],
   "source": [
    "with open(r'pkl/cities.pkl', 'rb') as infile:\n",
    "       cities = pickle.load(infile)\n",
    "\n",
    "print(len(cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "with open(r'pkl/abbr.pkl', 'rb') as infile:\n",
    "       abbr = pickle.load(infile)\n",
    "\n",
    "print(len(abbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Pass - Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>Petros Gazazyan North Hollywood CA Werkervarin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>Travis London Java Software Engineer Tucson AZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>Stephen A Kraft Mechanical Engineer Seattle WA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \n",
       "0  Petros Gazazyan North Hollywood CA Werkervarin...  \n",
       "1  Travis London Java Software Engineer Tucson AZ...  \n",
       "2  Stephen A Kraft Mechanical Engineer Seattle WA...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['resume_text']]\n",
    "df['resume_stopped'] = df['resume_text'].str.replace(r'''[^0-9a-zA-Z ]+''', '')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache stopwords first to reduce compute time\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "cachedStopWords += ['tot']\n",
    "cachedStopWords += cities\n",
    "cachedStopWords += abbr\n",
    "cachedStopWords = list(set(cachedStopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all text to lower case and separate into list\n",
    "df['resume_stopped'] = df['resume_stopped'].str.lower().str.split()\n",
    "\n",
    "# remove stopwords\n",
    "df['resume_stopped'] = df['resume_stopped'].apply(lambda x: ' '.join([item for item in x if item not in cachedStopWords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_pickle('pkl/df_stop.pkl')\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataframe from Pickle (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan werkervaring engineer structur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>java software engineer bereid overal naartoe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>kraft mechanical engineer bereid overal naarto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \n",
       "0  petros gazazyan werkervaring engineer structur...  \n",
       "1  java software engineer bereid overal naartoe t...  \n",
       "2  kraft mechanical engineer bereid overal naarto...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop.pkl')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12974248"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ct = ' '.join(df['resume_text'].tolist()).split()\n",
    "len(text_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8630319"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_ct = ' '.join(df['resume_stopped'].tolist()).split()\n",
    "len(stop_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aacademic', 'academic')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aaccounts', 'account')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aaerobic', 'aerobic')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aademy', 'academy')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aanalyst', 'analyst')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('aanalyzed', 'analyst')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('reports', 'report')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('reporting', 'report')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('engineering', 'engineer')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('engineers', 'engineer')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('services', 'service')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('servicing', 'service')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('systems', 'system')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('abandoned', 'abandon')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('abandoner', 'abandon')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('abandoning', 'abandon')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('abandonment', 'abandon')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('abandons', 'abandon')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('management', 'manage')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('manager', 'manage')\n",
    "\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('development', 'develop')\n",
    "df['resume_stopped'] = df['resume_stopped'].str.replace('developer', 'develop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = '''\n",
    "from nltk.corpus import words\n",
    "\n",
    "unique_wrds = sorted(set([x for x in stop_ct if x.isalpha()]))\n",
    "print(len(unique_wrds))\n",
    "non_en = []\n",
    "\n",
    "for wd in unique_wrds[:200]:\n",
    "    if wd not in words.words():\n",
    "        non_en.append(wd)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stems (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/port_stem.pkl'):\n",
    "    with open(r'pkl/port_stem.pkl', 'rb') as infile:\n",
    "       port_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    text = ' '.join(df['resume_stopped'].tolist())\n",
    "    \n",
    "    port_stem = []\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        port_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/lanc_stem.pkl'):\n",
    "    with open(r'pkl/lanc_stem.pkl', 'rb') as infile:\n",
    "       lanc_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    text = ' '.join(df['resume_stopped'].tolist())\n",
    "    \n",
    "    lanc_stem = []\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        lanc_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126305\n",
      "115243\n"
     ]
    }
   ],
   "source": [
    "print(len(set(port_stem)))\n",
    "print(len(set(lanc_stem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pkl(data, filename):\n",
    "    with open('{0}.pkl'.format(filename), 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_pkl(port_stem, 'pkl/port_stem')\n",
    "save_pkl(lanc_stem, 'pkl/lanc_stem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordcount(text_list, min_ct=3, most_common=30, get_all=False):\n",
    "    '''\n",
    "    returns => most common\n",
    "    '''\n",
    "    # get wordcount counter object\n",
    "    word_count = Counter(text_list)\n",
    "\n",
    "    # remove words that occur min_ct times or less\n",
    "    word_count = Counter({k:v for k, v in word_count.items() if v >= min_ct})\n",
    "\n",
    "    if get_all:\n",
    "        # return all\n",
    "        word_count = word_count.items()\n",
    "    else:\n",
    "        # limit wordcounts for visualization\n",
    "        word_count = word_count.most_common(most_common)\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_port_stem = get_wordcount(port_stem, 3, get_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_lanc_stem = get_wordcount(lanc_stem, 3, get_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt = ' '.join(df['resume_stopped']).split(' ')\n",
    "wordct = get_wordcount(txt, 3, get_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Lables, Counts of Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_count(word_count):\n",
    "    label = [lbl for lbl, ct in word_count]\n",
    "    count = [ct for lbl, ct in word_count]\n",
    "    return (label, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_bar(data_tup, title, file_name):\n",
    "    # make figure\n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ########## DATA ##############\n",
    "    lbl, ct = label_count(data_tup)\n",
    "    ##############################\n",
    "\n",
    "    # color\n",
    "    colors = sns.color_palette(\"BrBG\", len(lbl))\n",
    "\n",
    "    # plots\n",
    "    y_pos = np.arange(len(lbl))\n",
    "    ax.barh(y_pos, ct, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "    #plt.xlim(0,170000)\n",
    "    plt.ylim(-0.5,len(lbl))\n",
    "\n",
    "    # labels/titles\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title('{0} Word/Term Frequency'.format(title))\n",
    "    plt.xlabel('Word/Term Count')\n",
    "    plt.yticks(y_pos, lbl)\n",
    "    plt.ylabel('Word/Term')\n",
    "    plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "    # remove border\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_alpha(0.2)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_alpha(0.2)\n",
    "\n",
    "    # plot that biddy\n",
    "    plt.savefig('data/pics/{0}.png'.format(file_name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "wordct_port_stem_plt = get_wordcount(port_stem, 3, 30)\n",
    "wordct_lanc_stem_plt = get_wordcount(lanc_stem, 3, 30)\n",
    "wordct_plt = get_wordcount(txt, 3, 30)\n",
    "\n",
    "\n",
    "plot_bar(wordct_port_stem_plt, 'Porter Stem', 'porter_bar')\n",
    "plot_bar(wordct_lanc_stem_plt, 'Lancaster Stem', 'lancaster_bar')\n",
    "plot_bar(wordct_plt, 'Non-Stemmed', 'non-stem_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrack Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = lambda x: TextBlob(x).noun_phrases\n",
    "\n",
    "df['resume_nouns'] = df['resume_stopped']\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Noun Phrases Back to Text String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_to_str = lambda x: ' '.join(x)\n",
    "\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(lst_to_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_pickle('pkl/df_stop_noun.pkl')\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataframe from Pickle (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "      <th>resume_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan werkervaring engineer structur...</td>\n",
       "      <td>petros gazazyan engineer structural ttg engine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>java software engineer bereid overal naartoe t...</td>\n",
       "      <td>java software engineer bereid overal naartoe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>kraft mechanical engineer bereid overal naarto...</td>\n",
       "      <td>mechanical engineer bereid overal naartoe te v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \\\n",
       "0  petros gazazyan werkervaring engineer structur...   \n",
       "1  java software engineer bereid overal naartoe t...   \n",
       "2  kraft mechanical engineer bereid overal naarto...   \n",
       "\n",
       "                                        resume_nouns  \n",
       "0  petros gazazyan engineer structural ttg engine...  \n",
       "1  java software engineer bereid overal naartoe t...  \n",
       "2  mechanical engineer bereid overal naartoe te v...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrased Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_txt = ' '.join(df['resume_nouns']).split(' ')\n",
    "wordct_noun = get_wordcount(noun_txt, 2, get_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Frequency Chart of Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "wordct_noun_plt = get_wordcount(noun_txt, 2, 30)\n",
    "plot_bar(wordct_noun_plt, 'Noun Phrases', 'noun_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counter (Tuple) to Label List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lbl_lst(cntr):\n",
    "    lst = [x for x,y in cntr]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 99.49s\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "\n",
    "# convert resume texts to a sparse matrix of token counts\n",
    "ct_vect = CountVectorizer(ngram_range=(1, 4), max_df=0.90, min_df=2, max_features=n_features, stop_words='english')\n",
    "ct_vect_prep = ct_vect.fit_transform(df['resume_nouns'])\n",
    "\n",
    "print('Time: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 45.8s\n"
     ]
    }
   ],
   "source": [
    "lda_mdl = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0)\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "lda_mdl.fit(ct_vect_prep)\n",
    "\n",
    "print('Time: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pkl(lda_mdl, 'pkl/lda_mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature set: 1000\n",
      "Start of list: 10g, 11g, 9i, aanvullende, aanvullende informatie, aanvullende informatie skills, aanvullende informatie technical, aanvullende informatie technical skills, ability, able\n",
      "End of list: workflow, workflows, works, worldwide, wwwlinkedincom, xml, xp, year, years, years experience\n"
     ]
    }
   ],
   "source": [
    "# get feature names (topics) from model\n",
    "feat_names = ct_vect.get_feature_names()\n",
    "\n",
    "print('Length of feature set: {0}'.format(len(feat_names)))\n",
    "print('Start of list: ' + ', '.join(feat_names[:10]))\n",
    "print('End of list: ' + ', '.join(feat_names[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic {0}:\".format(i))\n",
    "        for wd in topic.argsort()[:-top_words - 1:-1]:\n",
    "            print('\\t{0}'.format(feature_names[wd]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "\tdata\n",
      "\tsoftware\n",
      "\tjava\n",
      "\tweb\n",
      "\tengineer\n",
      "\tpython\n",
      "\thadoop\n",
      "\tapplication\n",
      "\tanalysis\n",
      "\ttechnologies\n",
      "Topic 1:\n",
      "\tproject\n",
      "\ttest\n",
      "\tmanage\n",
      "\trequirements\n",
      "\tteam\n",
      "\tquality\n",
      "\tprocess\n",
      "\ttechnical\n",
      "\treport\n",
      "\tsoftware\n",
      "Topic 2:\n",
      "\tmanage\n",
      "\tanalysis\n",
      "\tsales\n",
      "\treport\n",
      "\tteam\n",
      "\tservice\n",
      "\tproject\n",
      "\tproduct\n",
      "\tsenior\n",
      "\tprocess\n",
      "Topic 3:\n",
      "\tengineer\n",
      "\tnetwork\n",
      "\tsupport\n",
      "\tmanage\n",
      "\tsecurity\n",
      "\tsoftware\n",
      "\ttechnical\n",
      "\tservice\n",
      "\thardware\n",
      "\tmaintenance\n",
      "Topic 4:\n",
      "\tanalysis\n",
      "\tsas\n",
      "\tweb\n",
      "\treport\n",
      "\tgoogle\n",
      "\tdata\n",
      "\tdigital\n",
      "\tcustomer\n",
      "\tmanage\n",
      "\tonline\n",
      "Topic 5:\n",
      "\tdata\n",
      "\tservice\n",
      "\tcustomer\n",
      "\tskills\n",
      "\tentry\n",
      "\toffice\n",
      "\tcustomer service\n",
      "\tdata entry\n",
      "\tmanage\n",
      "\tinformation\n",
      "Topic 6:\n",
      "\tdata\n",
      "\tanalysis\n",
      "\treport\n",
      "\tanalyst\n",
      "\tetl\n",
      "\tdata analysis\n",
      "\tsql\n",
      "\tbi\n",
      "\tintelligence\n",
      "\tdata analyst\n",
      "Topic 7:\n",
      "\tdata\n",
      "\thadoop\n",
      "\thive\n",
      "\tjava\n",
      "\texperience\n",
      "\tpig\n",
      "\thdfs\n",
      "\tweb\n",
      "\tapplication\n",
      "\tservice\n",
      "Topic 8:\n",
      "\tsql\n",
      "\tserver\n",
      "\tdatabase\n",
      "\tsql server\n",
      "\treport\n",
      "\tdata\n",
      "\tmanage\n",
      "\tenvironment\n",
      "\tservice\n",
      "\tapplication\n",
      "Topic 9:\n",
      "\tanalysis\n",
      "\tscientist\n",
      "\tdata\n",
      "\tlaboratory\n",
      "\tskills\n",
      "\tclinical\n",
      "\tcell\n",
      "\tengineer\n",
      "\tlab\n",
      "\tte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_mdl, feat_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 68.08s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(input='content', ngram_range=(1, 3), max_df=0.9, min_df=2, \n",
    "                max_features=n_features, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_vec_prep = tfidf_vec.fit_transform(df['resume_nouns'])\n",
    "\n",
    "print('Time: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19.28s\n"
     ]
    }
   ],
   "source": [
    "lda_mdl = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0)\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "lda_mdl.fit(tfidf_vec_prep)\n",
    "\n",
    "print('Time: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pkl(lda_mdl, 'pkl/lda_mdl_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of list: 10g, 11g, 9i, aanvullende, aanvullende informatie, aanvullende informatie skills, aanvullende informatie technical, ability, able, academic\n",
      "End of list: workflow, workflows, works, worldwide, wwwlinkedincom, xml, xp, year, years, years experience\n"
     ]
    }
   ],
   "source": [
    "# get feature names (topics) from model\n",
    "feat_names = tfidf_vec.get_feature_names()\n",
    "\n",
    "print('Start of list: ' + ', '.join(feat_names[:10]))\n",
    "print('End of list: ' + ', '.join(feat_names[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "\tengineer\n",
      "\tsales\n",
      "\telectrical\n",
      "\tapplications\n",
      "\tproduct\n",
      "\tscenarios\n",
      "\tsenior\n",
      "\tdesigns\n",
      "\tcommercial\n",
      "\trelease\n",
      "Topic 1:\n",
      "\tanalysis\n",
      "\tmanage\n",
      "\tdata\n",
      "\treport\n",
      "\tsales\n",
      "\tproject\n",
      "\tanalyst\n",
      "\tteam\n",
      "\tproduct\n",
      "\tsystem\n",
      "Topic 2:\n",
      "\tscientist\n",
      "\tlaboratory\n",
      "\tdata\n",
      "\tanalysis\n",
      "\tcell\n",
      "\tchemistry\n",
      "\tlab\n",
      "\tenvironmental\n",
      "\tengineer\n",
      "\tclinical\n",
      "Topic 3:\n",
      "\tengineer\n",
      "\tnetwork\n",
      "\tmaintenance\n",
      "\tsystem\n",
      "\telectrical\n",
      "\texperience\n",
      "\tproject\n",
      "\tmechanical\n",
      "\tmanage\n",
      "\trepair\n",
      "Topic 4:\n",
      "\tengineer\n",
      "\ttest\n",
      "\tequipment\n",
      "\tmechanical\n",
      "\tsystem\n",
      "\tquality\n",
      "\tanalysis\n",
      "\tvalidation\n",
      "\tmachine\n",
      "\tproduction\n",
      "Topic 5:\n",
      "\tengineer\n",
      "\tequipment\n",
      "\telectrical\n",
      "\tnetwork\n",
      "\tmechanical\n",
      "\ttechnician\n",
      "\tmaintenance\n",
      "\tsystem\n",
      "\trepair\n",
      "\tinstallation\n",
      "Topic 6:\n",
      "\tengineer\n",
      "\telectronics\n",
      "\tfund\n",
      "\tsystem\n",
      "\tproject\n",
      "\texperience\n",
      "\tmaintenance\n",
      "\tcomputer\n",
      "\ttest\n",
      "\tcommunication\n",
      "Topic 7:\n",
      "\thadoop\n",
      "\thive\n",
      "\tpig\n",
      "\tjava\n",
      "\thdfs\n",
      "\thbase\n",
      "\tsqoop\n",
      "\tmapreduce\n",
      "\tdata\n",
      "\toozie\n",
      "Topic 8:\n",
      "\tdata\n",
      "\tsql\n",
      "\tsystem\n",
      "\tserver\n",
      "\tmanage\n",
      "\tdatabase\n",
      "\tproject\n",
      "\tsoftware\n",
      "\tanalysis\n",
      "\treport\n",
      "Topic 9:\n",
      "\tentry\n",
      "\tdata entry\n",
      "\tdata\n",
      "\tcustomer\n",
      "\tservice\n",
      "\tcustomer service\n",
      "\tskills\n",
      "\toffice\n",
      "\tmanage\n",
      "\tcustomers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda_mdl, feat_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
