{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# default plot stying changes\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "colors = sns.color_palette('Set2',12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pw_file = 'credentials/pw.txt'\n",
    "if os.path.exists(pw_file): \n",
    "    with open(pw_file, 'r') as f:\n",
    "        email, indeed_pw = f.readline().strip().split(', ')\n",
    "        username, pia_pw = f.readline().strip().split(', ')\n",
    "        pub_ip, mongo_usr, mongo_usr_pw = f.readline().strip().split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to ec2 mongo client\n",
    "client = MongoClient('{0}:27017'.format(pub_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get reference to  resume_db\n",
    "db = client.resume_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authenticate user for database\n",
    "db.authenticate(mongo_usr, mongo_usr_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull MongoDB into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, no_id=True):\n",
    "    '''\n",
    "    db: mongodb already connected and authenticated\n",
    "    collection: desired collection in db\n",
    "    query: query filter\n",
    "    no_id: include mongos _id (False) or not (True)\n",
    "    return => pandas dataframe\n",
    "    '''\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# load database data into dataframe\n",
    "df = read_mongo(db, 'originals')\n",
    "\n",
    "print('Time to load data: {0}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['resume_text']]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(df['search_term'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Pass - Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df['resume_clean'] = df['resume_text'].str.replace(':|;', '')\n",
    "#df['resume_clean'] = df['resume_clean'].str.replace('.', '')\n",
    "#df['resume_clean'] = df['resume_clean'].str.replace(',', '')\n",
    "df['resume_stopped'] = df['resume_text'].str.replace(r'''[^0-9a-zA-Z ]+''', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache stopwords first to reduce compute time\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "cachedStopWords += ['tot']\n",
    "\n",
    "# convert all text to lower case and separate into list\n",
    "df['resume_stopped'] = df['resume_stopped'].str.lower().str.split()\n",
    "\n",
    "# remove stopwords\n",
    "df['resume_stopped'] = df['resume_stopped'].apply(lambda x: ' '.join([item for item in x if item not in cachedStopWords]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_ct = ' '.join(df['resume_text'].tolist()).split()\n",
    "len(text_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_ct = ' '.join(df['resume_stopped'].tolist()).split()\n",
    "len(stop_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stems (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/port_stem.pkl'):\n",
    "    with open(r'pkl/port_stem.pkl', 'rb') as infile:\n",
    "       port_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    text = ' '.join(df['resume_stopped'].tolist())\n",
    "    \n",
    "    port_stem = []\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        port_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if stemmed list already exists, load it\n",
    "if os.path.isfile('pkl/lanc_stem.pkl'):\n",
    "    with open(r'pkl/lanc_stem.pkl', 'rb') as infile:\n",
    "       lanc_stem = pickle.load(infile)\n",
    "else:\n",
    "# otherwise make the stemmed list\n",
    "    text = ' '.join(df['resume_stopped'].tolist())\n",
    "    \n",
    "    lanc_stem = []\n",
    "    stemmer = LancasterStemmer()\n",
    "\n",
    "    for word in TextBlob(text).words:\n",
    "        lanc_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(set(port_stem)))\n",
    "print(len(set(lanc_stem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Stemmed Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_pkl(data, filename):\n",
    "    with open('{0}.pkl'.format(filename), 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_pkl(port_stem, 'pkl/port_stem')\n",
    "save_pkl(lanc_stem, 'pkl/lanc_stem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordcount(text_list, min_ct=3, most_common=30, get_all=False):\n",
    "    '''\n",
    "    returns => most common\n",
    "    '''\n",
    "    # get wordcount counter object\n",
    "    word_count = Counter(text_list)\n",
    "\n",
    "    # remove words that occur min_ct times or less\n",
    "    word_count = Counter({k:v for k, v in word_count.items() if v >= min_ct})\n",
    "\n",
    "    if get_all:\n",
    "        # return all\n",
    "        word_count = word_count.items()\n",
    "    else:\n",
    "        # limit wordcounts for visualization\n",
    "        word_count = word_count.most_common(most_common)\n",
    "    \n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_port_stem = get_wordcount(port_stem, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordct_lanc_stem = get_wordcount(lanc_stem, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-Stemmed Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt = ' '.join(df['resume_stopped']).split(' ')\n",
    "wordct = get_wordcount(txt, 3, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Lables, Counts of Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_count(word_count):\n",
    "    label = [lbl for lbl, ct in word_count]\n",
    "    count = [ct for lbl, ct in word_count]\n",
    "    return (label, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_bar(data_tup, title, file_name):\n",
    "    # make figure\n",
    "    fig = plt.figure(figsize=(20,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ########## DATA ##############\n",
    "    lbl, ct = label_count(data_tup)\n",
    "    ##############################\n",
    "\n",
    "    # color\n",
    "    colors = sns.color_palette(\"BrBG\", len(lbl))\n",
    "\n",
    "    # plots\n",
    "    y_pos = np.arange(len(lbl))\n",
    "    ax.barh(y_pos, ct, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "    #plt.xlim(0,170000)\n",
    "    plt.ylim(-0.5,len(lbl))\n",
    "\n",
    "    # labels/titles\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title('{0} Word/Term Frequency'.format(title))\n",
    "    plt.xlabel('Word/Term Count')\n",
    "    plt.yticks(y_pos, lbl)\n",
    "    plt.ylabel('Word/Term')\n",
    "    plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "    # remove border\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_alpha(0.2)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_alpha(0.2)\n",
    "\n",
    "    # plot that biddy\n",
    "    plt.savefig('data/pics/{0}.png'.format(file_name), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bar(wordct_port_stem, 'Porter Stem', 'porter_bar')\n",
    "plot_bar(wordct_lanc_stem, 'Lancaster Stem', 'lancaster_bar')\n",
    "plot_bar(wordct, 'Non-Stemmed', 'non-stem_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrack Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = lambda x: TextBlob(x).noun_phrases\n",
    "\n",
    "df['resume_nouns'] = df['resume_stopped']\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Noun Phrases Back to Text String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst_to_str = lambda x: ' '.join(x)\n",
    "\n",
    "df['resume_nouns'] = df['resume_nouns'].apply(lst_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe to Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_pickle('pkl/df_stop_noun.pkl')\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataframe from Pickle (RESTART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Phrased Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noun_txt = ' '.join(df['resume_nouns']).split(' ')\n",
    "wordct_noun = get_wordcount(noun_txt, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordct_noun = get_wordcount(noun_txt, 2, get_all=True)\n",
    "wordct_noun_lst = [x for x,y in wordct_noun]\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "#list(sorted(wordct_noun, key=getKey, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT WORKING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Frequency Chart of Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_bar(list(wordct_noun), 'Noun Phrases', 'noun_bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counter (Tuple) to Label List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_lbl_lst(cntr):\n",
    "    lst = [x for x,y in cntr]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip (Select) Words from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# re-grab references to the stemmer objects\n",
    "port_stemmer = PorterStemmer()\n",
    "lanc_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns Only Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noun filter function to apply to dataframe\n",
    "noun_filter = lambda cell: ' '.join([x for x in cell.split() if x in wordct_noun_lst])\n",
    "\n",
    "# file name to save/load \n",
    "fname = 'df_noun_only'\n",
    "\n",
    "# if filtered dataframe already exists, load it\n",
    "if os.path.isfile('pkl/{0}.pkl'.format(fname)):\n",
    "    df.read_pickle('pkl/{0}.pkl'.format(fname))\n",
    "else:\n",
    "    # otherwise strip all words not in noun words list\n",
    "    df['resume_nouns'] = df['resume_nouns'].apply(noun_filter)\n",
    "    \n",
    "    # save to dataframe\n",
    "    df.to_pickle('pkl/{0}.pkl'.format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmed Only Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# porter stemmer filter function to apply to dataframe\n",
    "porter_filter = lambda cell: ' '.join([port_stemmer.stem(x) for x in cell.split() \n",
    "                                              if port_stemmer.stem(x) in count_lbl_lst(wordct_port_stem)])\n",
    "# file name to save/load \n",
    "fname = 'df_noun_port'\n",
    "\n",
    "# if filtered dataframe already exists, load it\n",
    "if os.path.isfile('pkl/{0}.pkl'.format(fname)):\n",
    "    df.read_pickle('pkl/{0}.pkl'.format(fname))\n",
    "else:\n",
    "    # otherwise strip all words not in porter stemmed words list\n",
    "    df['resume_porter'] = df['resume_stopped']\n",
    "    df['resume_porter'] = df['resume_porter'].apply(porter_filter)\n",
    "    \n",
    "    # save to dataframe\n",
    "    df.to_pickle('pkl/{0}.pkl'.format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmed Only Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lancaster stemmer filter function to apply to dataframe\n",
    "lancaster_filter = lambda cell: ' '.join([lanc_stemmer.stem(x) for x in cell.split() \n",
    "                                              if lanc_stemmer.stem(x) in count_lbl_lst(wordct_lanc_stem)])\n",
    "# file name to save/load \n",
    "fname = 'df_noun_port_lanc'\n",
    "\n",
    "# if filtered dataframe already exists, load it\n",
    "if os.path.isfile('pkl/{0}.pkl'.format(fname)):\n",
    "    df.read_pickle('pkl/{0}.pkl'.format(fname))\n",
    "else:\n",
    "    # otherwise strip all words not in lancaster stemmed words list\n",
    "    df['resume_lancaster'] = df['resume_stopped']\n",
    "    df['resume_lancaster'] = df['resume_lancaster'].apply(lancaster_filter)\n",
    "    \n",
    "    # save to dataframe\n",
    "    df.to_pickle('pkl/{0}.pkl'.format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# convert resume texts to a sparse matrix of token counts\n",
    "ct_vect = CountVectorizer(ngram_range=(1, 3), max_df=0.90, min_df=2, max_features=n_features, stop_words='english')\n",
    "ct_vect_prep = ct_vect.fit_transform(df['resume_text'])\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0)\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "lda_mdl.fit(ct_vect_prep)\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "\n",
    "# get feature names (topics) from model\n",
    "feat_names = ct_vect.get_feature_names()\n",
    "\n",
    "print('Start of list: ' + ', '.join(feat_names[:20]))\n",
    "print('End of list: ' + ', '.join(feat_names[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic {0}:\".format(i))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_words(lda_mdl, feat_names, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
