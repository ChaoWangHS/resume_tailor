{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from six import iteritems\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "      <th>resume_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan north hollywood ca werkervarin...</td>\n",
       "      <td>petros gazazyan hollywood ca design engineer s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \\\n",
       "0  petros gazazyan north hollywood ca werkervarin...   \n",
       "1  travis london java software engineer tucson az...   \n",
       "2  stephen kraft mechanical engineer seattle wa b...   \n",
       "\n",
       "                                        resume_nouns  \n",
       "0  petros gazazyan hollywood ca design engineer s...  \n",
       "1  travis london java software engineer tucson az...  \n",
       "2  stephen kraft mechanical engineer seattle wa b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Series to List of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['petros gazazyan hollywood ca design engineer structural ttg engineer pasadena ca december nonstructural equipment anchorage major southern california hospitals accordance asce cbc local codes extensive knowledge experience engineering programs design enercalc etabs hilti profis design remodel buildings beams columns foundations area work physical work remodel ensure work civil engineering student worker los angeles county department public works alhambra ca september publics needs transportation infrastructure project development division los angeles county engineers project managers geographic presentation data gis systems engineering reports documents fund multimillion dollar projects microsoft word excel access multiple projects bikeway coordination disaster reimbursement civil engineering california state university northridge northridge ca',\n",
       " 'travis london java software engineer tucson az bereid overal naartoe te verhuizen engineer contract senior software engineer tucson az september heden spinoff mentor graphics division contract software engineer fact work continuation service family customers stem employment mentor graphics senior software engineer mentor graphics wilsonville mei key accomplishments abilities tenor mentor graphics great organization strategic decision open source tools group development outcome business software support clients system administrator project technology tucson az juni mentor graphics young geek high school administrator position building management systems time project technology lead movement old c c tool eclipse java complete rewrite tool shell script ant script team dog food introduce j unit infrastructure project technology mentor graphics vaardigheden java eclipse ant shell management process development linux windows mysql c c system administration bugzilla sql documentation uml links http wwwlinkedincom londontravis https chroniclevitaecom people travis london profile aanvullende informatie accomplishments executable form uml tool http wwwxtumlorg graphical diagram gmf autosar tool mentor graphics xtuml eclipse tools graphical canvas interactive graphical tree tool xtuml models croatia support large customer train usage']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes = df['resume_nouns'].tolist()\n",
    "resumes[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents, remove stop words and words that only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in resume.split()] for resume in resumes]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# remove words that occur less than n times\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Token Count Dictionary to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:24:47,265 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2016-08-18 11:24:51,071 : INFO : adding document #10000 to Dictionary(42148 unique tokens: ['gerwien', 'globallogic', 'coaf', 'umatilla', 'collates']...)\n",
      "2016-08-18 11:24:53,322 : INFO : built Dictionary(47433 unique tokens: ['umatilla', 'collates', 'refactors', 'iaw', 'certificationsalesforcecom']...) from 17049 documents (total 6995164 corpus positions)\n",
      "2016-08-18 11:24:53,323 : INFO : saving Dictionary object under pkl/resume_token.dict, separately None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(47433 unique tokens: ['umatilla', 'collates', 'refactors', 'iaw', 'certificationsalesforcecom']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('pkl/resume_token.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokenized Resumes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:50:09,301 : INFO : storing corpus in Matrix Market format to pkl/resume_token.mm\n",
      "2016-08-18 11:50:09,302 : INFO : saving sparse matrix to pkl/resume_token.mm\n",
      "2016-08-18 11:50:09,302 : INFO : PROGRESS: saving document #0\n",
      "2016-08-18 11:50:09,633 : INFO : PROGRESS: saving document #1000\n",
      "2016-08-18 11:50:10,118 : INFO : PROGRESS: saving document #2000\n",
      "2016-08-18 11:50:10,571 : INFO : PROGRESS: saving document #3000\n",
      "2016-08-18 11:50:11,022 : INFO : PROGRESS: saving document #4000\n",
      "2016-08-18 11:50:11,431 : INFO : PROGRESS: saving document #5000\n",
      "2016-08-18 11:50:11,846 : INFO : PROGRESS: saving document #6000\n",
      "2016-08-18 11:50:12,204 : INFO : PROGRESS: saving document #7000\n",
      "2016-08-18 11:50:12,522 : INFO : PROGRESS: saving document #8000\n",
      "2016-08-18 11:50:12,873 : INFO : PROGRESS: saving document #9000\n",
      "2016-08-18 11:50:13,238 : INFO : PROGRESS: saving document #10000\n",
      "2016-08-18 11:50:13,616 : INFO : PROGRESS: saving document #11000\n",
      "2016-08-18 11:50:13,903 : INFO : PROGRESS: saving document #12000\n",
      "2016-08-18 11:50:14,174 : INFO : PROGRESS: saving document #13000\n",
      "2016-08-18 11:50:14,461 : INFO : PROGRESS: saving document #14000\n",
      "2016-08-18 11:50:14,787 : INFO : PROGRESS: saving document #15000\n",
      "2016-08-18 11:50:15,133 : INFO : PROGRESS: saving document #16000\n",
      "2016-08-18 11:50:15,473 : INFO : PROGRESS: saving document #17000\n",
      "2016-08-18 11:50:15,490 : INFO : saved 17049x47433 matrix, density=0.463% (3747585/808685217)\n",
      "2016-08-18 11:50:15,496 : INFO : saving MmCorpus index to pkl/resume_token.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 3), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 3), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 2), (44, 1), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 4), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('pkl/resume_token.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus[:1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming – One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace 'texts' with 'open(my_file.txt)' to read from files (one line in the file is a document)\n",
    "# or loop through and open each individual file (?)\n",
    "# either way, dictionary.doc2bow wants a list of words (aka - line.lower().split())\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in texts:\n",
    "            yield dictionary.doc2bow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doesn't load the corpus into memory!\n",
    "corpus_memory_friendly = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, to construct the dictionary without loading all texts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = '''\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:51:39,304 : INFO : loading Dictionary object from pkl/resume_token.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dictionary LOADED as 'dictionary'\n"
     ]
    }
   ],
   "source": [
    "# load tokenized dictionary\n",
    "if (os.path.exists('pkl/resume_token.dict')):\n",
    "    dictionary = corpora.Dictionary.load('pkl/resume_token.dict')\n",
    "    print('Tokenized dictionary LOADED as \\'dictionary\\'')\n",
    "else:\n",
    "    print('Tokenized dictionary NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:51:40,384 : INFO : loaded corpus index from pkl/resume_token.mm.index\n",
      "2016-08-18 11:51:40,385 : INFO : initializing corpus reader from pkl/resume_token.mm\n",
      "2016-08-18 11:51:40,385 : INFO : accepted corpus with 17049 documents, 47433 features, 3747585 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix LOADED as 'corpus'\n"
     ]
    }
   ],
   "source": [
    "# load sparse vector matrix\n",
    "if (os.path.exists('pkl/resume_token.mm')):\n",
    "    corpus = corpora.MmCorpus('pkl/resume_token.mm')\n",
    "    print('Sparse matrix LOADED as \\'corpus\\'')\n",
    "else:\n",
    "    print('Sparse matrix NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:52:50,113 : INFO : collecting document frequencies\n",
      "2016-08-18 11:52:50,115 : INFO : PROGRESS: processing document #0\n",
      "2016-08-18 11:52:56,897 : INFO : PROGRESS: processing document #10000\n",
      "2016-08-18 11:53:00,848 : INFO : calculating IDF weights for 17049 documents and 47432 features (3747585 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# step 1 -- initialize a model\n",
    "tfidf_mdl = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `model[corpus]` only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensim’s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.09697439731236943), (1, 0.01994670625868208), (2, 0.11683622359629328), (3, 0.030254916666750908), (4, 0.050117949458092725), (5, 0.008687422725094847), (6, 0.062312898778185806), (7, 0.12291201188606497), (8, 0.10929528811163892), (9, 0.21606180788503235), (10, 0.1948861293470026), (11, 0.07715334861543517), (12, 0.18141552217773346), (13, 0.03443172309251477), (14, 0.06266765793155121), (15, 0.1579713910568025), (16, 0.05363696542306602), (17, 0.034978307404207425), (18, 0.17271063699473888), (19, 0.09806275559265303), (20, 0.026715620471933557), (21, 0.09579563734550717), (22, 0.17271063699473888), (23, 0.1286770479452766), (24, 0.05708005820057115), (25, 0.1576960892928475), (26, 0.014893087963931375), (27, 0.0031983449109230845), (28, 0.1987386650313829), (29, 0.029200285539392586), (30, 0.04382620878455877), (31, 0.015930497729036763), (32, 0.07833740220461406), (33, 0.12493293764695687), (34, 0.05345672339257628), (35, 0.0527571713622434), (36, 0.09384214238200134), (37, 0.2019386760209232), (38, 0.17583871580139643), (39, 0.07898569552840125), (40, 0.04971824158457874), (41, 0.07984585576025988), (42, 0.21606180788503235), (43, 0.1402332972469724), (44, 0.13105904113007688), (45, 0.06284823684064099), (46, 0.05866903715703634), (47, 0.09466997898226184), (48, 0.15210717095434634), (49, 0.08070256008242034), (50, 0.17756298649335314), (51, 0.0822686529533966), (52, 0.04440923553310443), (53, 0.05739861655196701), (54, 0.045344515077901736), (55, 0.021270653194615553), (56, 0.048372634082003985), (57, 0.037039715475002524), (58, 0.0531597851000379), (59, 0.045120610943777374), (60, 0.028111498583329404), (61, 0.07767268184331622), (62, 0.3138052508689102), (63, 0.030631563282604485), (64, 0.13934441983670573), (65, 0.1886052760673623), (66, 0.01962639515959034), (67, 0.08211146996805817), (68, 0.1068605134662751), (69, 0.04045726274429159), (70, 0.052313337065352114), (71, 0.02822915085917675), (72, 0.08176138158112022), (73, 0.030626570372331647), (74, 0.11036399302565564), (75, 0.2978876517466357), (76, 0.03176680616923904), (77, 0.12583615067715762), (78, 0.01822612629924122)]\n"
     ]
    }
   ],
   "source": [
    "# step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf_mdl[corpus]\n",
    "\n",
    "# view one resume\n",
    "for doc in corpus_tfidf[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:58:52,709 : INFO : using serial LSI version on this node\n",
      "2016-08-18 11:58:52,710 : INFO : updating model with new documents\n",
      "2016-08-18 11:59:07,640 : INFO : preparing a new chunk of documents\n",
      "2016-08-18 11:59:08,595 : INFO : using 100 extra samples and 2 power iterations\n",
      "2016-08-18 11:59:08,596 : INFO : 1st phase: constructing (47433, 110) action matrix\n",
      "2016-08-18 11:59:09,046 : INFO : orthonormalizing (47433, 110) action matrix\n",
      "2016-08-18 11:59:12,156 : INFO : 2nd phase: running dense svd on (110, 17049) matrix\n",
      "2016-08-18 11:59:12,535 : INFO : computing the final decomposition\n",
      "2016-08-18 11:59:12,536 : INFO : keeping 10 factors (discarding 56.308% of energy spectrum)\n",
      "2016-08-18 11:59:12,567 : INFO : processed documents up to #17049\n",
      "2016-08-18 11:59:12,572 : INFO : topic #0(24.717): 0.137*\"sql\" + 0.129*\"business\" + 0.108*\"analytics\" + 0.104*\"hadoop\" + 0.100*\"oracle\" + 0.098*\"server\" + 0.092*\"project\" + 0.088*\"sales\" + 0.087*\"hive\" + 0.085*\"database\"\n",
      "2016-08-18 11:59:12,574 : INFO : topic #1(14.917): -0.291*\"hadoop\" + -0.281*\"hive\" + -0.203*\"hdfs\" + -0.200*\"pig\" + -0.178*\"java\" + -0.137*\"sqoop\" + -0.130*\"hbase\" + -0.126*\"sql\" + -0.117*\"oracle\" + -0.114*\"mapreduce\"\n",
      "2016-08-18 11:59:12,575 : INFO : topic #2(11.035): -0.212*\"hive\" + -0.205*\"hadoop\" + 0.194*\"sql\" + -0.163*\"hdfs\" + -0.159*\"pig\" + -0.152*\"entry\" + 0.150*\"etl\" + 0.140*\"informatica\" + 0.126*\"oracle\" + 0.120*\"server\"\n",
      "2016-08-18 11:59:12,576 : INFO : topic #3(10.260): 0.224*\"analytics\" + -0.178*\"entry\" + 0.147*\"media\" + 0.127*\"google\" + 0.125*\"digital\" + 0.117*\"strategy\" + 0.111*\"hadoop\" + -0.109*\"sql\" + 0.108*\"market\" + -0.106*\"server\"\n",
      "2016-08-18 11:59:12,578 : INFO : topic #4(8.914): 0.235*\"sas\" + 0.179*\"scientist\" + 0.174*\"laboratory\" + 0.146*\"research\" + -0.140*\"sales\" + 0.127*\"r\" + 0.123*\"python\" + 0.122*\"cell\" + 0.105*\"clinical\" + 0.104*\"statistical\"\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-18 11:59:12,816 : INFO : topic #0(24.717): 0.137*\"sql\" + 0.129*\"business\" + 0.108*\"analytics\" + 0.104*\"hadoop\" + 0.100*\"oracle\" + 0.098*\"server\" + 0.092*\"project\" + 0.088*\"sales\" + 0.087*\"hive\" + 0.085*\"database\"\n",
      "2016-08-18 11:59:12,818 : INFO : topic #1(14.917): -0.291*\"hadoop\" + -0.281*\"hive\" + -0.203*\"hdfs\" + -0.200*\"pig\" + -0.178*\"java\" + -0.137*\"sqoop\" + -0.130*\"hbase\" + -0.126*\"sql\" + -0.117*\"oracle\" + -0.114*\"mapreduce\"\n",
      "2016-08-18 11:59:12,820 : INFO : topic #2(11.035): -0.212*\"hive\" + -0.205*\"hadoop\" + 0.194*\"sql\" + -0.163*\"hdfs\" + -0.159*\"pig\" + -0.152*\"entry\" + 0.150*\"etl\" + 0.140*\"informatica\" + 0.126*\"oracle\" + 0.120*\"server\"\n",
      "2016-08-18 11:59:12,821 : INFO : topic #3(10.260): 0.224*\"analytics\" + -0.178*\"entry\" + 0.147*\"media\" + 0.127*\"google\" + 0.125*\"digital\" + 0.117*\"strategy\" + 0.111*\"hadoop\" + -0.109*\"sql\" + 0.108*\"market\" + -0.106*\"server\"\n",
      "2016-08-18 11:59:12,823 : INFO : topic #4(8.914): 0.235*\"sas\" + 0.179*\"scientist\" + 0.174*\"laboratory\" + 0.146*\"research\" + -0.140*\"sales\" + 0.127*\"r\" + 0.123*\"python\" + 0.122*\"cell\" + 0.105*\"clinical\" + 0.104*\"statistical\"\n",
      "2016-08-18 11:59:12,824 : INFO : topic #5(8.630): -0.254*\"network\" + -0.249*\"cisco\" + 0.241*\"sas\" + -0.193*\"engineer\" + -0.126*\"switches\" + -0.124*\"security\" + 0.118*\"sql\" + -0.111*\"hardware\" + -0.103*\"routers\" + -0.099*\"servers\"\n",
      "2016-08-18 11:59:12,825 : INFO : topic #6(8.420): 0.221*\"media\" + 0.215*\"google\" + 0.181*\"social\" + 0.156*\"web\" + 0.146*\"digital\" + 0.137*\"sql\" + 0.134*\"seo\" + 0.130*\"server\" + -0.128*\"financial\" + -0.121*\"laboratory\"\n",
      "2016-08-18 11:59:12,829 : INFO : topic #7(7.745): 0.383*\"sas\" + -0.175*\"laboratory\" + 0.159*\"cisco\" + 0.149*\"network\" + -0.148*\"media\" + -0.118*\"cell\" + 0.118*\"tx\" + -0.115*\"informatica\" + -0.110*\"clinical\" + 0.109*\"python\"\n",
      "2016-08-18 11:59:12,831 : INFO : topic #8(7.174): -0.265*\"ca\" + 0.173*\"test\" + -0.171*\"informatica\" + -0.149*\"san\" + 0.147*\"ny\" + -0.147*\"architect\" + -0.133*\"warehouse\" + 0.130*\"york\" + -0.128*\"etl\" + -0.122*\"bi\"\n",
      "2016-08-18 11:59:12,832 : INFO : topic #9(7.115): -0.266*\"cisco\" + 0.227*\"ca\" + -0.196*\"network\" + -0.192*\"sas\" + 0.171*\"tx\" + -0.135*\"switches\" + 0.135*\"san\" + 0.122*\"java\" + 0.120*\"c\" + 0.117*\"engineer\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.137*\"sql\" + 0.129*\"business\" + 0.108*\"analytics\" + 0.104*\"hadoop\" + 0.100*\"oracle\" + 0.098*\"server\" + 0.092*\"project\" + 0.088*\"sales\" + 0.087*\"hive\" + 0.085*\"database\"'),\n",
       " (1,\n",
       "  '-0.291*\"hadoop\" + -0.281*\"hive\" + -0.203*\"hdfs\" + -0.200*\"pig\" + -0.178*\"java\" + -0.137*\"sqoop\" + -0.130*\"hbase\" + -0.126*\"sql\" + -0.117*\"oracle\" + -0.114*\"mapreduce\"'),\n",
       " (2,\n",
       "  '-0.212*\"hive\" + -0.205*\"hadoop\" + 0.194*\"sql\" + -0.163*\"hdfs\" + -0.159*\"pig\" + -0.152*\"entry\" + 0.150*\"etl\" + 0.140*\"informatica\" + 0.126*\"oracle\" + 0.120*\"server\"'),\n",
       " (3,\n",
       "  '0.224*\"analytics\" + -0.178*\"entry\" + 0.147*\"media\" + 0.127*\"google\" + 0.125*\"digital\" + 0.117*\"strategy\" + 0.111*\"hadoop\" + -0.109*\"sql\" + 0.108*\"market\" + -0.106*\"server\"'),\n",
       " (4,\n",
       "  '0.235*\"sas\" + 0.179*\"scientist\" + 0.174*\"laboratory\" + 0.146*\"research\" + -0.140*\"sales\" + 0.127*\"r\" + 0.123*\"python\" + 0.122*\"cell\" + 0.105*\"clinical\" + 0.104*\"statistical\"'),\n",
       " (5,\n",
       "  '-0.254*\"network\" + -0.249*\"cisco\" + 0.241*\"sas\" + -0.193*\"engineer\" + -0.126*\"switches\" + -0.124*\"security\" + 0.118*\"sql\" + -0.111*\"hardware\" + -0.103*\"routers\" + -0.099*\"servers\"'),\n",
       " (6,\n",
       "  '0.221*\"media\" + 0.215*\"google\" + 0.181*\"social\" + 0.156*\"web\" + 0.146*\"digital\" + 0.137*\"sql\" + 0.134*\"seo\" + 0.130*\"server\" + -0.128*\"financial\" + -0.121*\"laboratory\"'),\n",
       " (7,\n",
       "  '0.383*\"sas\" + -0.175*\"laboratory\" + 0.159*\"cisco\" + 0.149*\"network\" + -0.148*\"media\" + -0.118*\"cell\" + 0.118*\"tx\" + -0.115*\"informatica\" + -0.110*\"clinical\" + 0.109*\"python\"'),\n",
       " (8,\n",
       "  '-0.265*\"ca\" + 0.173*\"test\" + -0.171*\"informatica\" + -0.149*\"san\" + 0.147*\"ny\" + -0.147*\"architect\" + -0.133*\"warehouse\" + 0.130*\"york\" + -0.128*\"etl\" + -0.122*\"bi\"'),\n",
       " (9,\n",
       "  '-0.266*\"cisco\" + 0.227*\"ca\" + -0.196*\"network\" + -0.192*\"sas\" + 0.171*\"tx\" + -0.135*\"switches\" + 0.135*\"san\" + 0.122*\"java\" + 0.120*\"c\" + 0.117*\"engineer\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the topics are printed to log\n",
    "lsi.print_topics(num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "...     print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
