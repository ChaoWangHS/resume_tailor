{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from six import iteritems\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>resume_stopped</th>\n",
       "      <th>resume_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Petros Gazazyan North Hollywood, CA Werkervari...</td>\n",
       "      <td>petros gazazyan north hollywood ca werkervarin...</td>\n",
       "      <td>petros gazazyan hollywood ca design engineer s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travis London Java Software Engineer Tucson, A...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "      <td>travis london java software engineer tucson az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen A. Kraft Mechanical Engineer Seattle, ...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "      <td>stephen kraft mechanical engineer seattle wa b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  Petros Gazazyan North Hollywood, CA Werkervari...   \n",
       "1  Travis London Java Software Engineer Tucson, A...   \n",
       "2  Stephen A. Kraft Mechanical Engineer Seattle, ...   \n",
       "\n",
       "                                      resume_stopped  \\\n",
       "0  petros gazazyan north hollywood ca werkervarin...   \n",
       "1  travis london java software engineer tucson az...   \n",
       "2  stephen kraft mechanical engineer seattle wa b...   \n",
       "\n",
       "                                        resume_nouns  \n",
       "0  petros gazazyan hollywood ca design engineer s...  \n",
       "1  travis london java software engineer tucson az...  \n",
       "2  stephen kraft mechanical engineer seattle wa b...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('pkl/df_stop_noun.pkl')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Series to List of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['petros gazazyan hollywood ca design engineer structural ttg engineer pasadena ca december nonstructural equipment anchorage major southern california hospitals accordance asce cbc local codes extensive knowledge experience engineering programs design enercalc etabs hilti profis design remodel buildings beams columns foundations area work physical work remodel ensure work civil engineering student worker los angeles county department public works alhambra ca september publics needs transportation infrastructure project development division los angeles county engineers project managers geographic presentation data gis systems engineering reports documents fund multimillion dollar projects microsoft word excel access multiple projects bikeway coordination disaster reimbursement civil engineering california state university northridge northridge ca']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes = df['resume_nouns'].tolist()\n",
    "resumes[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Strings to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the documents, remove stop words and words that only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in resume.split()] for resume in resumes]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# remove words that occur less than n times\n",
    "texts = [[token for token in text if frequency[token] > 2] for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Token Count Dictionary to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(47433 unique tokens: ['zonar', 'thotampali', 'amite', 'junior', 'mclennan']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# store the dictionary, for future reference\n",
    "dictionary.save('pkl/resume_token.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tokenized Resumes to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 2), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 1), (73, 4), (74, 1), (75, 4), (76, 1), (77, 2), (78, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('pkl/resume_token.mm', corpus)  # store to disk, for later use\n",
    "for c in corpus[:1]:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming â€“ One Document at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace 'texts' with 'open(my_file.txt)' to read from files (one line in the file is a document)\n",
    "# or loop through and open each individual file (?)\n",
    "# either way, dictionary.doc2bow wants a list of words (aka - line.lower().split())\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in texts:\n",
    "            yield dictionary.doc2bow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doesn't load the corpus into memory!\n",
    "corpus_memory_friendly = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly, to construct the dictionary without loading all texts into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = '''\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()\n",
    "print(dictionary)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dictionary LOADED as 'dictionary'\n"
     ]
    }
   ],
   "source": [
    "# load tokenized dictionary\n",
    "if (os.path.exists('pkl/resume_token.dict')):\n",
    "    dictionary = corpora.Dictionary.load('pkl/resume_token.dict')\n",
    "    print('Tokenized dictionary LOADED as \\'dictionary\\'')\n",
    "else:\n",
    "    print('Tokenized dictionary NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix LOADED as 'corpus'\n"
     ]
    }
   ],
   "source": [
    "# load sparse vector matrix\n",
    "if (os.path.exists('pkl/resume_token.mm')):\n",
    "    corpus = corpora.MmCorpus('pkl/resume_token.mm')\n",
    "    print('Sparse matrix LOADED as \\'corpus\\'')\n",
    "else:\n",
    "    print('Sparse matrix NOT FOUND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 1 -- initialize a model\n",
    "tfidf_mdl = models.TfidfModel(corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `model[corpus]` only creates a wrapper around the old corpus document stream â€“ actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus], because that would mean storing the result in main memory, and that contradicts gensimâ€™s objective of memory-indepedence. If you will be iterating over the transformed corpus_transformed multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.17271063699473888), (1, 0.08226865295339658), (2, 0.10686051346627509), (3, 0.09697439731236941), (4, 0.053636965423066015), (5, 0.0527571713622434), (6, 0.09466997898226183), (7, 0.058669037157036336), (8, 0.053456723392576275), (9, 0.028229150859176745), (10, 0.11683622359629327), (11, 0.17756298649335311), (12, 0.15797139105680247), (13, 0.20193867602092316), (14, 0.037039715475002524), (15, 0.04534451507790173), (16, 0.07984585576025988), (17, 0.043826208784558764), (18, 0.21606180788503232), (19, 0.09384214238200132), (20, 0.019946706258682077), (21, 0.12493293764695686), (22, 0.19873866503138288), (23, 0.0776726818433162), (24, 0.04512061094377737), (25, 0.057398616551967006), (26, 0.05231333706535211), (27, 0.12291201188606496), (28, 0.019626395159590335), (29, 0.08211146996805817), (30, 0.09806275559265301), (31, 0.06266765793155121), (32, 0.049718241584578736), (33, 0.008687422725094845), (34, 0.07898569552840123), (35, 0.08070256008242034), (36, 0.18141552217773343), (37, 0.014893087963931373), (38, 0.10929528811163891), (39, 0.04440923553310442), (40, 0.14023329724697237), (41, 0.04837263408200398), (42, 0.21606180788503232), (43, 0.02920028553939258), (44, 0.13105904113007688), (45, 0.031766806169239034), (46, 0.05011794945809272), (47, 0.01593049772903676), (48, 0.19488612934700258), (49, 0.0281114985833294), (50, 0.04045726274429158), (51, 0.3138052508689102), (52, 0.15769608929284748), (53, 0.0817613815811202), (54, 0.003198344910923084), (55, 0.03443172309251477), (56, 0.053159785100037896), (57, 0.0623128987781858), (58, 0.03497830740420742), (59, 0.057080058200571146), (60, 0.07715334861543516), (61, 0.09579563734550715), (62, 0.11036399302565562), (63, 0.06284823684064098), (64, 0.030626570372331643), (65, 0.17271063699473888), (66, 0.02127065319461555), (67, 0.01822612629924122), (68, 0.2978876517466356), (69, 0.026715620471933554), (70, 0.030631563282604482), (71, 0.1886052760673623), (72, 0.030254916666750904), (73, 0.12583615067715762), (74, 0.17583871580139643), (75, 0.13934441983670573), (76, 0.12867704794527657), (77, 0.15210717095434634), (78, 0.07833740220461406)]\n"
     ]
    }
   ],
   "source": [
    "# step 2 -- use the model to transform vectors\n",
    "corpus_tfidf = tfidf_mdl[corpus]\n",
    "\n",
    "# view one resume\n",
    "for doc in corpus_tfidf[:1]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "n_features = 1000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(input='content', ngram_range=(1, 3), max_df=0.9, min_df=2, \n",
    "                max_features=n_features, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "tfidf_vec_prep = tfidf_vec.fit_transform(resumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "km = KMeans(n_clusters=8, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "km_mdl = km.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='cosine', algorithm='brute', \n",
    "                leaf_size=30, p=None, random_state=None)\n",
    "\n",
    "dbscan_mdl = dbscan.fit_predict(tfidf_vec_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     2,     4, ..., 17044, 17045, 17047])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_topics = 100\n",
    "\n",
    "# initialize an LSI transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.137*\"sql\" + 0.129*\"business\" + 0.108*\"analytics\" + 0.104*\"hadoop\" + 0.100*\"oracle\" + 0.098*\"server\" + 0.092*\"project\" + 0.088*\"sales\" + 0.087*\"hive\" + 0.085*\"database\"'),\n",
       " (1,\n",
       "  '0.291*\"hadoop\" + 0.281*\"hive\" + 0.203*\"hdfs\" + 0.200*\"pig\" + 0.178*\"java\" + 0.137*\"sqoop\" + 0.130*\"hbase\" + 0.126*\"sql\" + 0.117*\"oracle\" + 0.114*\"mapreduce\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the topics are printed to log\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc in corpus_lsi[800]: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    pass\n",
    "    #print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.save('pkl/lsi_mdl.lsi')\n",
    "lsi = models.LsiModel.load('pkl/lsi_mdl.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = models.LdaModel(corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6,\n",
      "  '0.011*data + 0.009*design + 0.007*mn + 0.007*ok + 0.006*production + '\n",
      "  '0.005*sirna + 0.005*development + 0.005*multiplex + 0.005*art + '\n",
      "  '0.004*oklahoma'),\n",
      " (3,\n",
      "  '0.024*data + 0.022*analysis + 0.021*university + 0.013*research + '\n",
      "  '0.012*science + 0.011*r + 0.009*scientist + 0.007*statistical + '\n",
      "  '0.006*python + 0.006*model'),\n",
      " (2,\n",
      "  '0.020*management + 0.017*project + 0.013*process + 0.010*analysis + '\n",
      "  '0.010*business + 0.010*team + 0.009*development + 0.008*system + '\n",
      "  '0.008*support + 0.008*training'),\n",
      " (9,\n",
      "  '0.017*support + 0.017*security + 0.016*software + 0.016*systems + '\n",
      "  '0.014*data + 0.012*system + 0.009*management + 0.009*information + '\n",
      "  '0.008*network + 0.008*technical'),\n",
      " (18,\n",
      "  '0.043*ca + 0.015*san + 0.014*c + 0.014*engineering + 0.014*software + '\n",
      "  '0.012*data + 0.012*engineer + 0.010*design + 0.008*development + '\n",
      "  '0.007*california'),\n",
      " (15,\n",
      "  '0.067*data + 0.022*sql + 0.013*business + 0.012*database + 0.011*oracle + '\n",
      "  '0.010*etl + 0.010*design + 0.009*server + 0.008*system + 0.008*project'),\n",
      " (10,\n",
      "  '0.038*data + 0.023*hadoop + 0.019*hive + 0.014*java + 0.014*pig + '\n",
      "  '0.012*experience + 0.011*hdfs + 0.007*web + 0.007*big + 0.007*sqoop'),\n",
      " (16,\n",
      "  '0.034*network + 0.024*cisco + 0.010*switches + 0.009*ip + '\n",
      "  '0.009*configuration + 0.008*data + 0.008*routers + 0.008*experience + '\n",
      "  '0.008*security + 0.007*servers'),\n",
      " (17,\n",
      "  '0.012*data + 0.012*equipment + 0.008*systems + 0.007*technical + '\n",
      "  '0.007*support + 0.006*isolation + 0.006*technician + 0.006*system + '\n",
      "  '0.006*network + 0.005*maintenance'),\n",
      " (0,\n",
      "  '0.034*sap + 0.016*wa + 0.012*design + 0.012*data + 0.011*engineering + '\n",
      "  '0.008*system + 0.008*seattle + 0.008*process + 0.007*product + '\n",
      "  '0.007*project')]\n"
     ]
    }
   ],
   "source": [
    "lda_mdl.top_topics\n",
    "pprint(lda_mdl.print_topics(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdp = models.hdpmodel.HdpModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdp_topics = hdp.show_topics(topics=5, topn=5, log=False, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdp.optimal_ordering()\n",
    "hdp_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
