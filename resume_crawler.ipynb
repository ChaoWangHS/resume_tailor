{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "'''\n",
    "\n",
    "import html5lib\n",
    "from bs4 import BeautifulSoup\n",
    "from random import randint\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind SOCKS5 Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import socket\n",
    "import socks\n",
    "\n",
    "PORT = 1080\n",
    "\n",
    "# no authentication\n",
    "# socks.set_default_proxy(socks.SOCKS5, \"localhost\")\n",
    "\n",
    "# with Authentication\n",
    "socks.set_default_proxy(socks.SOCKS5, \"proxy-nl.privateinternetaccess.com\",PORT,True,\"x6122622\",\"A8yZzRo6GH\")\n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def csv_to_list(file):\n",
    "    with open('{0}'.format(file), 'rt') as f:\n",
    "        return [row[0] for row in csv.reader(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Down Resumes from Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def html_to_text(id_name):\n",
    "    url = 'http://www.indeed.com{0}co=us/pdf'.format(id_name)\n",
    "    \n",
    "    ### SLEEP\n",
    "    sleep(randint(1,5))\n",
    "    ### SLEEP\n",
    "    \n",
    "    # request content from site\n",
    "    try:\n",
    "        html = requests.get(url, stream=True)\n",
    "        \n",
    "        data = html.text.encode('utf-8').decode('ascii', 'ignore')\n",
    "        soup = BeautifulSoup(data, 'lxml')\n",
    "\n",
    "        # get only resume content\n",
    "        res = soup.find('div', {'id': 'resume'})\n",
    "        res = \" \".join(res.strings)\n",
    "\n",
    "        # remove special characters\n",
    "        res = re.sub(r'([-/*&%()@#$^])', ' ', res)\n",
    "\n",
    "        cleaner = ['\\xa0']\n",
    "        for c in cleaner:\n",
    "            res = res.replace(c,'')\n",
    "\n",
    "        # remove multiple white spaces\n",
    "        res = re.sub(' +',' ', res)\n",
    "    except:\n",
    "        res = 'FAIL'\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_text(title):\n",
    "    csv_file = 'data/{0}.csv'.format(title)\n",
    "    \n",
    "    if os.path.isfile(csv_file):\n",
    "        \n",
    "        # loop through name id's\n",
    "        name_ids = csv_to_list(csv_file)\n",
    "        for i,name in enumerate(name_ids):\n",
    "            # start new timer\n",
    "            start_time = time()\n",
    "            \n",
    "            # CALL TO GET RESUME FROM SITE\n",
    "            text_res = html_to_text(name)\n",
    "            if text_res != 'FAIL':\n",
    "                ofile = 'data/pdfs/{0}_{1}.txt'.format(title, i)\n",
    "\n",
    "                # delete old version\n",
    "                if os.path.exists(ofile):\n",
    "                    os.remove(ofile)\n",
    "\n",
    "                # write to file\n",
    "                with open(ofile, 'w') as f:\n",
    "                        f.write(text_res)\n",
    "\n",
    "                # log results to file to show progress/status\n",
    "                length = len(name_ids) - i\n",
    "                with open('data/logfile.txt', 'a') as f:\n",
    "                    elapsed_time = time() - start_time\n",
    "                    log = '{0}: time {1}s\\t remaining {2} \\n'.format(title, str(round(elapsed_time,3)), length)\n",
    "                    f.write(log)\n",
    "            else:\n",
    "                with open('data/stopped.txt', 'a') as f:\n",
    "                    elapsed_time = time() - start_time\n",
    "                    log = '{0}: time {1}s\\t remaining {2} (list id {3}) \\n'.format(title, \n",
    "                                                                str(round(elapsed_time,3)), length, i)\n",
    "                    f.write(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Resume Link csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_titles():\n",
    "    all_files = glob.glob('data/*.csv')\n",
    "    copy_files = glob.glob('data/* copy.csv')\n",
    "    full_files = list(set(all_files) - set(copy_files))\n",
    "    titles = [re.search('data\\/(.*?)\\.csv',x).group(1) for x in full_files]\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Download Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scientist', 'analytics', 'big_data', 'data_analysis', 'data']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_titles()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delete old version\n",
    "if os.path.exists('data/logfile.txt'):\n",
    "    os.remove('data/logfile.txt')\n",
    "\n",
    "# loop through all the titles of the id files downloaded\n",
    "for file in all_titles()[1:]:\n",
    "    download_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log In to Indeed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "driver.get('https://secure.indeed.com/account/login?service=rex&hl=en_US&co=US&cfb=2&continue=http%3A%2F%2Fwww.indeed.com%2Fresumes')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "driver.set_window_size(1080,720)\n",
    "try:\n",
    "    username_form = driver.find_element_by_id('signin_email')\n",
    "    username_form.send_keys(\"bryantbiggs@gmail.com\")\n",
    "    username_form.send_keys(Keys.TAB)\n",
    "except Exception:\n",
    "    driver.save_screenshot('email.png')\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "driver.set_window_size(1080,720)\n",
    "try:\n",
    "    password_form = driver.find_element_by_id('signin_password')\n",
    "    password_form.send_keys(\"Yamaharx1!\")\n",
    "    password_form.send_keys(Keys.ENTER)\n",
    "except Exception:\n",
    "    driver.save_screenshot('password.png')\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
