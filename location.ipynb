{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from pymongo import MongoClient\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# default plot stying changes\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_palette(\"Set2\")\n",
    "colors = sns.color_palette('Set2',12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pw_file = 'pw.txt'\n",
    "if os.path.exists(pw_file): \n",
    "    with open(pw_file, 'r') as f:\n",
    "        email, indeed_pw = f.readline().strip().split(', ')\n",
    "        username, pia_pw = f.readline().strip().split(', ')\n",
    "        pub_ip, mongo_usr, mongo_usr_pw = f.readline().strip().split(', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to ec2 mongo client\n",
    "client = MongoClient('{0}:27017'.format(pub_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get reference to  resume_db\n",
    "db = client.resume_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# authenticate user for database\n",
    "db.authenticate(mongo_usr, mongo_usr_pw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull MongoDB into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, no_id=True):\n",
    "    '''\n",
    "    db: mongodb already connected and authenticated\n",
    "    collection: desired collection in db\n",
    "    query: query filter\n",
    "    no_id: include mongos _id (False) or not (True)\n",
    "    return => pandas dataframe\n",
    "    '''\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# load database data into dataframe\n",
    "df = read_mongo(db, 'originals')\n",
    "\n",
    "print('Time to load data: {0}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(df['search_term'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Pass - Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['resume_clean'] = df['resume_text'].str.replace(':|;', '')\n",
    "df['resume_clean'] = df['resume_clean'].str.replace('.', '')\n",
    "df['resume_clean'] = df['resume_clean'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache stopwords first to reduce compute time\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "cachedStopWords += ['tot']\n",
    "\n",
    "# convert all text to lower case and separate into list\n",
    "df['resume_stopped'] = df['resume_clean'].str.lower().str.split()\n",
    "\n",
    "# remove stopwords\n",
    "df['resume_stopped'] = df['resume_stopped'].apply(lambda x: ' '.join([item for item in x if item not in cachedStopWords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[['resume_text', 'resume_clean', 'resume_stopped']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = ' '.join(df['resume_text'].tolist()).split()\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean = ' '.join(df['resume_clean'].tolist()).split()\n",
    "len(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stp = ' '.join(df['resume_stopped'].tolist()).split()\n",
    "len(stp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem With Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ' '.join(df['resume_stopped'].tolist())\n",
    "\n",
    "# porter stemmer\n",
    "port_stem = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in TextBlob(text).words:\n",
    "    port_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lancaster stemmer\n",
    "lanc_stem = []\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "for word in TextBlob(text).words:\n",
    "    lanc_stem.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np = TextBlob(text).noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nouns only\n",
    "\n",
    "port_noun = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "for word in TextBlob(text).noun_phrases:\n",
    "    port_noun.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wordcount for stemmed corpus\n",
    "wordct_stem = Counter(stem)\n",
    "\n",
    "# limit wordcounts for visualization\n",
    "wordct_stem = wordct_stem.most_common(60)\n",
    "print(len(wordct_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wordcount for unstemmed corpus\n",
    "wordct = Counter(' '.join(df['resume_stopped']).split(' '))\n",
    "\n",
    "# limit wordcounts for visualization\n",
    "wordct = wordct.most_common(60)\n",
    "print(len(wordct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# un-stemmed most common for bar chart\n",
    "labels = [lbl for lbl, ct in wordct]\n",
    "count = [ct for lbl, ct in wordct]\n",
    "\n",
    "# stemmed most common for bar chart\n",
    "labels_stem = [lbl for lbl, ct in wordct_stem]\n",
    "count_stem = [ct for lbl, ct in wordct_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", len(labels))\n",
    "\n",
    "# plots\n",
    "y_pos = np.arange(len(labels))\n",
    "ax.barh(y_pos, count, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "#plt.xlim(0,170000)\n",
    "plt.ylim(-1,len(labels))\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Un-Stemmed Word/Term Frequency')\n",
    "plt.xlabel('Word/Term Count')\n",
    "plt.yticks(y_pos, labels)\n",
    "plt.ylabel('Word/Term')\n",
    "plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "ax.spines[\"bottom\"].set_visible(False) \n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "#ax.xaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "#ax.yaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "\n",
    "# plot that biddy\n",
    "plt.tight_layout()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", len(labels_stem))\n",
    "\n",
    "# plots\n",
    "y_pos = np.arange(len(labels_stem))\n",
    "ax.barh(y_pos, count_stem, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "#plt.xlim(0,170000)\n",
    "plt.ylim(-1,len(labels))\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Stemmed Word/Term Frequency')\n",
    "plt.xlabel('Word/Term Count')\n",
    "plt.yticks(y_pos, labels)\n",
    "plt.ylabel('Word/Term')\n",
    "plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "ax.spines[\"bottom\"].set_visible(False) \n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "#ax.xaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "#ax.yaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "\n",
    "# plot that biddy\n",
    "plt.tight_layout()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = df['resume_text'].tolist()\n",
    "clean = df['resume_clean'].tolist()\n",
    "stopped = df['resume_stopped'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text = vectorizer.fit_transform(text)\n",
    "print('Raw resume text number of features: {0}'.format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_clean = vectorizer.fit_transform(clean)\n",
    "print('Cleaned resume text number of features: {0}'.format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_stop = vectorizer.fit_transform(stopped)\n",
    "print('Stopped resume text number of features: {0}'.format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stopped, stop_words='english')\n",
    "x_stop = vectorizer.fit_transform(stopped)\n",
    "print('Stopped resume text number of features: {0}'.format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect_stopped = vectorizer.get_feature_names()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stem_vect_stop = []\n",
    "for i in vect_stopped:\n",
    "    stem_vect_stop.append(stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(stem_vect_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['stem'] = df['resume_stopped'].apply(lambda x: stemmer.stem(x))\n",
    "#df['stem'] = df['resume_stopped'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get wordcounts\n",
    "wordcount = Counter(' '.join(df['resume_stopped']).split(' '))\n",
    "\n",
    "# limit wordcounts for visualization\n",
    "wordcount = wordcount.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = [lbl for lbl, ct in wordcount]\n",
    "count = [ct for lbl, ct in wordcount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# color\n",
    "colors = sns.color_palette(\"BrBG\", len(labels))\n",
    "\n",
    "# plots\n",
    "y_pos = np.arange(len(labels))\n",
    "ax.barh(y_pos, count, align='center', color=colors, edgecolor=colors)\n",
    "\n",
    "#plt.xlim(0,170000)\n",
    "plt.ylim(-1,30)\n",
    "\n",
    "# labels/titles\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Word/Term Frequency')\n",
    "plt.xlabel('Word/Term Count')\n",
    "plt.yticks(y_pos, labels)\n",
    "plt.ylabel('Word/Term')\n",
    "plt.xticks(np.linspace(0,180000, 13))\n",
    "\n",
    "# remove border\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False) \n",
    "ax.spines[\"bottom\"].set_visible(False) \n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# show grid\n",
    "#ax.xaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "#ax.yaxis.grid(True, alpha=0.2, linestyle='--') \n",
    "\n",
    "# plot that biddy\n",
    "plt.tight_layout()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(w.lower() for w in df['resume_stopped'])\n",
    "#fd.plot(10)\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time()\n",
    "\n",
    "# convert resume texts to a sparse matrix of token counts\n",
    "ct_vect = CountVectorizer(ngram_range=(1, 3), max_df=0.90, min_df=2, max_features=n_features, stop_words='english')\n",
    "ct_vect_prep = ct_vect.fit_transform(df['resume_text'])\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_mdl = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', \n",
    "                                learning_offset=50., random_state=0)\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "lda_mdl.fit(ct_vect_prep)\n",
    "\n",
    "print('Time to count vectorize data: {0:.4}s'.format(time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Topics in LDA model:\")\n",
    "\n",
    "# get feature names (topics) from model\n",
    "feat_names = ct_vect.get_feature_names()\n",
    "\n",
    "print('Start of list: ' + ', '.join(feat_names[:20]))\n",
    "print('End of list: ' + ', '.join(feat_names[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Top Words in Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic {0}:\".format(i))\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_words(lda_mdl, feat_names, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)[source]¶"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
